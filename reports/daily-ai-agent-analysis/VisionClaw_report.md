# é¡¹ç›®åˆ†æžæŠ¥å‘Š: sseanliu/VisionClaw

## é¡¹ç›®æ¦‚è§ˆ
- **é¡¹ç›®åœ°å€**: https://github.com/sseanliu/VisionClaw
- **é¡¹ç›®æè¿°**: Real-time AI assistant for Meta Ray-Ban smart glasses -- voice + vision + agentic actions via Gemini Live and OpenClaw
- **ä¸»è¦è¯­è¨€**: Unknown
- **æ˜Ÿæ ‡æ•°**: 855
- **å¤åˆ»æ•°**: 149
- **å¼€æ”¾é—®é¢˜**: 7
- **è®¸å¯è¯**: NOASSERTION
- **æœ€åŽæ›´æ–°**: 2026-02-13T08:58:17Z
- **ä¸»é¢˜æ ‡ç­¾**: 

## ä¸€å¥è¯ä»‹ç»
sseanliu/VisionClaw æ˜¯ä¸€ä¸ªåŸºäºŽ Unknown çš„ å¼€å‘è€…å·¥å…· / ä»£ç è¾…åŠ© é¡¹ç›®ï¼Œæå‡å¼€å‘æ•ˆçŽ‡ï¼Œæ‹¥æœ‰ 855 ä¸ªæ˜Ÿæ ‡ã€‚

## æ ¸å¿ƒäº®ç‚¹
å¤šæ¨¡æ€å¤„ç†

## åº”ç”¨é¢†åŸŸ
å¼€å‘è€…å·¥å…· / ä»£ç è¾…åŠ©

## æŠ€æœ¯æ ˆ
- æŠ€æœ¯æ ˆæœªæ˜Žç¡®æŒ‡å®š

## æ ¸å¿ƒç‰¹æ€§
- å¤šæ¨¡æ€å¤„ç†

## æ‰©å±•èƒ½åŠ›
ä½Žè‡³ä¸­ - ä»£ç ä¸­æ£€æµ‹åˆ°æ‰©å±•èƒ½åŠ›

## æ‰§è¡Œæµç¨‹è§£æž
åŸºäºŽä»£ç åº“åˆ†æžï¼Œè¯¥é¡¹ç›®çš„ä¸»è¦æ‰§è¡Œæµç¨‹å¯èƒ½åŒ…æ‹¬ï¼š
1. åˆå§‹åŒ–é˜¶æ®µï¼šæ ¹æ®é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°è®¾ç½®è¿è¡ŒçŽ¯å¢ƒ
2. è¾“å…¥å¤„ç†ï¼šæŽ¥æ”¶ç”¨æˆ·è¾“å…¥æˆ–ä»»åŠ¡æŒ‡ä»¤
3. ä»£ç†å¾ªçŽ¯ï¼šæ‰§è¡ŒAIæŽ¨ç†ã€å·¥å…·è°ƒç”¨ã€è¡ŒåŠ¨è§„åˆ’ç­‰
4. è¾“å‡ºå¤„ç†ï¼šç”Ÿæˆå“åº”æˆ–æ‰§è¡Œç»“æžœ
5. è®°å¿†/çŠ¶æ€ç®¡ç†ï¼šæ›´æ–°å†…éƒ¨çŠ¶æ€æˆ–è®°å¿†ç³»ç»Ÿ

(å…·ä½“æ‰§è¡Œæµç¨‹éœ€å‚è€ƒæºä»£ç å®žçŽ°)

## ä»“åº“ç»“æž„
```text
.
./assets
./samples
./samples/CameraAccess
./samples/CameraAccess/CameraAccess
./samples/CameraAccess/CameraAccessTests
./samples/CameraAccess/CameraAccess.xcodeproj
```

## ä¼˜åŠ¿åˆ†æž
- æŒç»­å¢žé•¿ (855 â­)
- æ´»è·ƒç¤¾åŒº (>149 å¤åˆ»)
- ç»´æŠ¤è‰¯å¥½ (ä½Žå¼€æ”¾é—®é¢˜æ•°: 7)
- è®¸å¯è¯æ¸…æ™° (NOASSERTION)

## æ½œåœ¨ä¸è¶³
- æ–‡æ¡£æœ‰é™
- ç¼ºå°‘ç¤ºä¾‹
- æ— å¯è§æµ‹è¯•å¥—ä»¶

## READMEé¢„è§ˆ
```markdown
# VisionClaw ðŸ¦ž+ðŸ˜Ž

![VisionClaw](assets/teaserimage.png)

A real-time AI assistant for Meta Ray-Ban smart glasses. See what you see, hear what you say, and take actions on your behalf -- all through voice.

![Cover](assets/cover.png)

Built on [Meta Wearables DAT SDK](https://github.com/facebook/meta-wearables-dat-ios) + [Gemini Live API](https://ai.google.dev/gemini-api/docs/live) + [OpenClaw](https://github.com/nichochar/openclaw) (optional).

## What It Does

Put on your glasses, tap the AI button, and talk:

- **"What am I looking at?"** -- Gemini sees through your glasses camera and describes the scene
- **"Add milk to my shopping list"** -- delegates to OpenClaw, which adds it via your connected apps
- **"Send a message to John saying I'll be late"** -- routes through OpenClaw to WhatsApp/Telegram/iMessage
- **"Search for the best coffee shops nearby"** -- web search via OpenClaw, results spoken back

The glasses camera streams at ~1fps to Gemini for visual context, while audio flows bidirectionally in real-time.

## How It Works

![How It Works](assets/how.png)

```
Meta Ray-Ban Glasses (or iPhone camera)
       |
       | video frames + mic audio
       v
iOS App (this project)
       |
       | JPEG frames (~1fps) + PCM audio (16kHz)
       v
Gemini Live API (WebSocket)
       |
       |-- Audio response (PCM 24kHz) --> iOS App --> Speaker
       |-- Tool calls (execute) -------> iOS App --> OpenClaw Gateway
       |                                                  |
       |                                                  v
       |                                          56+ skills: web search,
       |                                          messaging, smart home,
       |                                          notes, reminders, etc.
       |                                                  |
       |<---- Tool response (text) <----- iOS App <-------+
       |
       v
  Gemini speaks the result
```
```

## è¡¥å……è¯´æ˜Ž
- æ–‡æ¡£å®Œå–„åº¦: ç¼ºå¤±æˆ–ç®€å•
- æµ‹è¯•è¦†ç›–åº¦: ç¼ºå¤±æˆ–ç®€å•
- ç¤ºä¾‹ä¸°å¯Œåº¦: ç¼ºå¤±æˆ–ç®€å•

---
*åˆ†æžæ—¶é—´: 2026-02-13*
